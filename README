## Imputation Pipeline For MTOB
We report chrF improvements across all examples of gpt-3.5-turbo gpt-4, and select examples for Claude 2.
This repository outlines the steps to replicate the imputation process and recreate our published results.

### Clone MTOB
The first step involves cloning the MTOB benchmark under the current directory. The mtob folder must be placed directly under the root of the project, i.e. mtob_impute. The materials of MTOB can be found https://lukemelas.github.io/mtob/
Follow the setup steps as prescribed in the README of MTOB in https://github.com/lukemelas/mtob/blob/main/README.md
1. Ensure you have correctly setup installing requirements.txt.
2. Ensure the contents of the kalamang dataset has been unzipped. Our imputations will directly modify the wordlist. We ask that the integrity of the dataset be maintained to prevent potential test set contamination into training data of LLMs as outlined in MTOB, i.e. avoid publishing to public repositories with the test sets.
3. Ensure the correct API keys have been added. If MTOB doesn't pick up your .env, it may be useful to add the keys to the venv or manually enter the keys in the parameters of the LLM instantiation (i.e. openai_api_key argument)
4. We note that within the wordlist.json, there are markings that may need to be removed, such as "textscpl", "textscsg", "textscex", "textscin" and "endletter. In our experiments we manually removed these instances in the wordlist.

### Run Imputation Pipeline
1. Ensure you have OpenAI API keys setup in a .env file.
2. run preprocessing.py
3. run query_builder.py to build queries for GPT.
4. To impute, run query_llm.py, this will automatically update the wordlist in mtob.

### Generate Translations
Run mtob/baselines/main.py with appropriate parameters, i.e. --use_reference_wordlist --use_reference_sentences --use_reference_book_passages. Note that after the imputation step, the wordlist.json has been imputed.

### Evaluation
Run mtob/baselines/eval.py on the results file to compute the metrics.
While conducting our evaluation with eval.py in mtob/baselines, we found it useful to comment out `assert len(rr_preds) == sum([pred != new_pred for pred, new_pred in zip(preds, new_preds, strict=True)])` in the `load()` function which appeared to arbitrarily prevent eval.py from running. This line is intended to help with validate reruns with a more "forceful" prompt when GPT-4 refuses to provide translation and can be commented out with little impact on results.
