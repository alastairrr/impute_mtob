## Imputation Pipeline For MTOB
We report chrF improvements across all examples of gpt-3.5-turbo gpt-4, and select examples for Claude 2.
In our imputation experiments, we focus on translations from Kalamang to English.
This repository outlines the steps to replicate the imputation process and recreate our published results.

### Clone MTOB
The first step involves cloning the MTOB benchmark under the current directory. 

`git clone https://github.com/lukemelas/mtob.git`

The mtob folder must be placed directly under the root of the project, i.e. mtob_impute. The materials of MTOB can be found https://lukemelas.github.io/mtob/
Follow the setup steps as prescribed in the README of MTOB in https://github.com/lukemelas/mtob/blob/main/README.md
1. Ensure you have correctly setup installing requirements.txt.
2. Ensure the contents of the kalamang dataset has been unzipped. Our imputations will directly modify the wordlist. We ask that the integrity of the dataset be maintained to prevent potential test set contamination into training data of LLMs as outlined in MTOB, i.e. avoid publishing to public repositories with the test sets.
3. Ensure the correct API keys have been added to .env.
4. We note that within the wordlist.json, there are markings that may need to be removed, such as "textscpl", "textscsg", "textscex", "textscin" and "endletter. In our experiments we manually removed these instances in the wordlist.

### Run Imputation Pipeline
1. Ensure you have OpenAI API keys setup in a .env file. See .env.example for the example.
2. run preprocessing.py to produce a temporary wordlist with affix alterations. This is so that we can detect affixes within words in the subword segmentations process.
3. run query_builder.py to build queries for GPT.
4. To impute, run query_llm.py, this will automatically update the wordlist in mtob.

### Generate Translations
Run mtob/baselines/main.py with appropriate parameters, i.e. --use_reference_wordlist --use_reference_sentences --use_reference_book_passages. Example: 

`python main.py --direction ke --model_type openai --model_name gpt-3.5-turbo --use_reference_wordlist`

Note that after the imputation step, the wordlist.json has been imputed. Refer to https://github.com/lukemelas/mtob/blob/main/README.md on available parameters.

If MTOB doesn't pick up your .env, it may be useful to add the keys to the virtual env or manually enter the keys in the parameters of the LLM instantiation in mtob/baselines/main.py (i.e. openai_api_key argument).

The output translations are found in mtob/baselines/outputs/ke.

At times, running on GPT-4 or Claude 2.0 may result in LLMs refusing to translate or adding supplementary information. In our experiments, we re-run the failed queries manually with forceful_manual_llm.py, using the prescribed approach from MTOB with a more forceful prompt.

### Evaluation
Run mtob/baselines/eval.py on the results file to compute the metrics on translations generated by main.py. Example:

`python eval.py --direction ke --input_file ./outputs/ke/results_test_openai_gpt-3.5-turbo_temp_0.05_reference_wordlist_2.json`

The evaluations are found in mtob/baselines/outputs/eval-ke. While LLM temperature is set to 0.05 for lower stochasticness in LLM outputs, we expect small deviations from reported results in chrF.

While conducting our evaluation with eval.py in mtob/baselines, we found it useful to comment out `assert len(rr_preds) == sum([pred != new_pred for pred, new_pred in zip(preds, new_preds, strict=True)])` in the `load()` function which appeared to arbitrarily prevent eval.py from running. This line is intended to help with validate reruns with a more "forceful" prompt when GPT-4 refuses to provide translation and can be commented out with no expected impact on results.
